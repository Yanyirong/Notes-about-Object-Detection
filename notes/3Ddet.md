# Object Detection 3D
## 3D目标检测的预测目标
1. 位置(x,y,z)，通常是中心点在真实世界的坐标，以米为单位
2. 尺寸(w,l,h)，物体的三维尺寸，以米为单位
3. 朝向$\theta$，通常为俯视图上物体前进方向和x轴的夹角
4. 物体类别（汽车，行人）或属性（静止、运动）
5. 有些场景需要预测速度

更一般的情况下，物体可能在yaw,pitch,roll三个方向上都有旋转，在加上上下、左右
前后三个方向的移动，总共有6个运动自由度，称为6-DoF(Degree of Freedom)

在自动驾驶场景里，默认pitch,roll为0，因此只需要yaw，称为4-DoF

## 3D检测的问题和思路
3D和2D不同的地方就是要多出一个对深度信息的估计，图像上没有深度信息，但是3D检测要求给出深度的信息，所以必须要有所补充

1. 方法一：数据，使用3D传感器来获取周围环境的物理信息，让数据本身就包含深度信息。
   这样的做法通常更准确，但是需要增加额外的设备和成本，数据形态和图像不同，需要特定的模型和算法。
   **即基于点云的算法**
2. 方法二：从算法入手，基于图像，让算法推理出深度。
   这样的做法设备简单、成本低，算法层面可以借鉴很多的2D的模型，但是问题本身存在一定的病态性
   **即基于单目相机或多目相机的纯视觉算法**
3. 方法三：融合图像和点云数据进行预测
   实践中需要考虑多传感器的标定和对齐
   **即多模态方法**

## 3D传感器与数据
### LiDAR
><https://en.wikipedia.org/wiki/Lidar>

基本原理：发射激光脉冲，物体反射后被接收器接收，测量发射接收的时间差即可计算距离，这个时间差叫做TOF（time of flight）。

![](https://upload.wikimedia.org/wikipedia/commons/f/f1/20200501_Time_of_flight.svg)

让激光头旋转起来就能实现水平360度的测量

这一系列得到的点的集合称为点云，是对物体表面的空间采样

为了覆盖不同的俯仰角度，可以将激光头在垂直方向排成一列，这些激光头同步旋转，可以探测到不同俯仰角度上的物体，由于每个激光头的俯仰角在制造过程中是确定且已知的，所以可以计算出相应的三维坐标



### 点云数据的特点
1. 点云是点的集合，是非结构化的数据，点与点之间没有顺序或位置上的关系，将点云矩阵按行打乱后表示的是同样的点云。相比之下RGB图像是结构化的，像素之间会有上下左右的位置关系
2. 点云提供三维结构的信息，但是不包含逻辑和纹理
3. 点云在近处密度大，远处密度小，室外场景更明显，一些远处的小物体可能只有几个点，但是图像会有很高的分辨率

### 3D传感器

**激光雷达**：使用激光测量物体的距离，常用于室外场景，精度较高，但激光容易受天气等因素影响
**毫米波雷达**：Radar,使用毫米波（1-10mm的电磁波）测量物体到雷达的距离，常用于室外，微波的穿透性强于光波，因此不易受天气和环境的影响，精度比LiDAR低，此外可借助多普勒效应测量物体速度
**RGB-D相机**：通过红外线测量物体到相机的距离，可以在拍摄RGB彩图的同时给出单通道深度图，红外线在室外受光影响较大，通常用于室内

## 坐标系
图像坐标系：左上角原点，XY轴向右向下，单位为像素，（u,v）
相机坐标系：相机光轴为Z轴，X轴水平向右，Y轴竖直向下，单位为米，(x',y',z')
世界坐标系：关注的物体所在的坐标系，是已知的，单位为米，(x,y,z)

我们要做的是将世界坐标系变换到图像坐标系

世界坐标系到相机坐标系的变换是刚体变换，平移+旋转，用外参矩阵表示

从相机坐标系到图像坐标系，使用的是小孔成像模型，用内参矩阵表示

><https://blog.csdn.net/chentravelling/article/details/53558096>

## 点云网络的设计
总体的思路：对点云使用主干网络提取特征，通过任务头得到结果。

### 点云处理方法
点云是不能直接使用卷积处理的，有以下的方法

1. 给点云赋予空间结构：空间栅格化，基于空间形成3D栅格或者地面形成2D栅格，将点云以某种方式嵌入栅格中，形成栅格化的数据，之后可以基于卷积网络或Transformer来提取特征
2. 直接处理点云，**PointNet和PointNet++模型**

### PointNet
><https://arxiv.org/abs/1612.00593>

![](http://stanford.edu/~rqi/pointnet/images/pointnet.jpg)

理论：因为点云的性质，因此网络的函数应该是对称的，PointNet使用MLP计算每个点的特征，再用max-pooling得到全局的特征

物体的形状和坐标系的选取无关，所以点云的特征对应刚性变换应该具有不变性

一个简单的实现方法：对点云数据进行预处理，通过仿射变换将坐标转换到一个常规坐标系下，再计算特征。依照这个思路，PointNet网络中加入了刚性变化的模块，以实现对刚性变换的不变性。

PointNet使用一个子网络T-Net预测刚性变换的参数，再将变换矩阵应用到所有空间点或特征点上，以实现整个网络对刚性变换的不变性

PointNet具有稳定性。其结构中的MLP应用于单点，池化基于所有点的特征，因此只能得到单点的特征和全局的特征，没有介于中间尺度的特征，PointNet是没有逐级抽象的能力的

### PointNet++
><https://arxiv.org/abs/1706.02413>
引入了Set Abstraction来解决层次化的问题。

过程是先进行下采样得到中心点，每个中心点取邻域进行聚合，基于每个领域中的点计算PointNet特征，再与原始的点云坐标拼接

下采样用的是FPS（Farthest Point Sampling），FPS基于点云的空间结构，可以更好地撑起点云

Set Abstraction相当于一个对点云进行一次抽象的操作，可以堆叠这个操作，实现层次化处理

![](http://stanford.edu/~rqi/pointnet2/images/pnpp.jpg)

同时，根据点云的密度不均匀，PointNet++还引入了Multi-scale grouping（MSG），具体来说是在Set Abstraction的聚合步骤中使用不同的邻域半径进行PointNet的特征计算，再将不同的尺度的特征拼接起来，作为中心点的特征，输出到下一层

## 基于点云数据的3D检测算法
### 基本思路
* 方法一：基于体素(Voxel)
    在三维空间中划分格子，将点云以某种方式嵌入到格子中，得到一个三维的特征体，之后可以借鉴2D检测的流程，使用主干网络和特征头组成的模型产生预测的结构，主干网络通常基于3D卷积网络
    **代表作：VoxelNet,SECOND...**
* 方法二：基于点云的鸟瞰图
    与体素的方法类似，但只在地面上划分格子，高度维不划分，可以将点云直接转化为2D特征图，之后基于2D检测算法产生预测框，由于预测目标有所区别，需要更改回归分支
    **代表作：Pixor,Complex-YOLO,PointPillars,CenterPoint等**
* 方法三：基于点
    使用PointNet++，直接基于非结构化的点云产生特征，再基于点的特征产生预测框
    **代表作：Point RCNN**
### 基于空间栅格的方法
基本思路：
1. 将空间在水平竖直深度三个方向上划分格子，每个格子称为体素（类比像素），或仅在地面划分格子，形成一个个柱体
2. 在体素内使用类PointNet模型提取点云特征，将不规则的点云转化为规则的2D特征图或3D特征体
3. 基于卷积网络和图像检测算法产生检测框

#### VoxelNet
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*ge0k7Nb4-6Zhz7R4a-2ZBA.png)
整体流程：
1. 将空间划分为体素，在体素内使用VFE提取局部点云特征，得到三维的特征体
2. 将特征送入3D卷积网络，进一步提高表达能力，将最终输出在竖直方向压缩，得到2D特征图
3. 将2D特征图送入PRN网络产生3D框的预测

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*wqOvy8tYQ6TqAb-Ncgkjow.png)

